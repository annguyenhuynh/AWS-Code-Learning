{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import StructType,StructField, StringType\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "import boto3\n",
    "from datetime import datetime, timedelta\n",
    "import pytz\n",
    "from dateutil import tz\n",
    "\n",
    "utc=pytz.UTC\n",
    "\n",
    "sc = SparkContext()\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "\n",
    "##Config\n",
    "\n",
    "#input definitions\n",
    "\n",
    "addin_bucket = \"credence-cloudwatch-metrics-addins\"\n",
    "data_bucket =\"credence-cloudwatch-metrics-raw\"\n",
    "report_bucket = \"credence-cloudwatch-metrics-reports\"\n",
    "main_prefix = \"ec2-metrics/\"\n",
    "\n",
    "# Sets up automatic date generation for file sorting and destination naming\n",
    "now = datetime.now()\n",
    "first_day_of_current_month = datetime(now.year, now.month, 1)\n",
    "\n",
    "last_day_of_previous_month = first_day_of_current_month - timedelta(days=1)\n",
    "month = last_day_of_previous_month.strftime('%b')\n",
    "\n",
    "year_num = now.year\n",
    "month_num = now.month-1 # two digit month'08'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accounts for January getting last month of previous year\n",
    "\n",
    "if now.month == 1:\n",
    "    month_num = 12\n",
    "    year_num = year_num - 1\n",
    "\n",
    "# Accounts for the required two digits on one digit months\n",
    "elif now.month < 11:\n",
    "    month_num = f\"0{str(month_num)}\"\n",
    "\n",
    "# Ensures string type for month and year\n",
    "year_num = str(year_num)\n",
    "month_num = str(month_num)\n",
    "\n",
    " \n",
    "\n",
    "# month_num = '11'\n",
    "\n",
    "# month = 'Nov'\n",
    "\n",
    "print(month, month_num)\n",
    "rnd = 0 # Number of digits to round to\n",
    "\n",
    "start_date = f\"{year_num}-{month_num}-01\"\n",
    "end_date = f\"{year_num}-{month_num}-31\" # No need to change\n",
    "\n",
    "# Patching has to be entered manualy every month\n",
    "start_ts = \"2025-03-04 00:08:00\" # Patching start/end time \n",
    "end_ts = \"2025-03-04 03:13:00\"\n",
    "\n",
    "##Config 8:30EST == 13:30GMT\n",
    "\n",
    "patch_start_dt = datetime.strptime(start_ts, \"%Y-%m-%d %H:%M:%S\")\n",
    "patch_end_dt = datetime.strptime(end_ts, \"%Y-%m-%d %H:%M:%S\")\n",
    "total_patch_time = (patch_end_dt - patch_start_dt).total_seconds()/3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dest_report = f\"s3://{report_bucket}/Uptime_Reports/Processed/{year_num}/{month_num}\" #daas_daasebusgexd93_\n",
    "dest_report_raw = f\"s3://{report_bucket}/Raw/{year_num}/{month_num}\"\n",
    "\n",
    "tenant_list = [\"daas\", \"daas-api\",\"daas-aiml\"]\n",
    "env_list = [\"prod\"] # api-prod\n",
    "\n",
    "print(f\"tenant list: {tenant_list}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_dict = {}\n",
    "##Config\n",
    "\n",
    "col_list = [\n",
    "    'instance_id',\n",
    "    'instance_name',\n",
    "    'instance_type',\n",
    "    'environment',\n",
    "    'DATE',\n",
    "    'TIME',\n",
    "]\n",
    "\n",
    " \n",
    "for tenant in tenant_list:\n",
    "    print(f\"Environment list: {env_list}\")\n",
    "    tenant_dict = {}\n",
    "\n",
    "    # #Iterate through each environment within the tenant, get csv list, prep to iterate and insert dataframe into dict\n",
    "    for env in env_list:\n",
    "        print(data_bucket + '/' + main_prefix + tenant + '/' + env)\n",
    "\n",
    "        tenant_dict[env] = {'Raw': (spark.read.options(header='True', inferSchema='True', delimiter=',')\n",
    "            .csv(f\"s3://{data_bucket}/{main_prefix}{tenant}/{env}/\")\n",
    "            .filter(col(\"instance_type\") != \"terminated_instance\")\n",
    "            .filter(col(\"DATE\").between(start_date,end_date))\n",
    "            .withColumn(\"input_file_name\",input_file_name())\n",
    "            .withColumn(\"dt_string\", concat('DATE', lit(' '), \"TIME\"))\n",
    "            .withColumn(\"timestamp\", col(\"dt_string\").cast(\"timestamp\"))\n",
    "            .withColumn(\"environment\", regexp_replace(\"environment\",\"-\",\"_\"))\n",
    "            .withColumn(\"environment\", regexp_replace(\"environment\",\"piee_\",\"\"))\n",
    "            .filter((\n",
    "\n",
    "                (col(\"environment\") != \"daas_prod\")\n",
    "\n",
    "                | (\n",
    "\n",
    "                    (col(\"environment\") == \"daas_prod\")\n",
    "\n",
    "                    & (~col(\"timestamp\").between(patch_start_dt, patch_end_dt))\n",
    "\n",
    "                    )\n",
    "\n",
    "                )\n",
    "\n",
    "            )\n",
    "\n",
    "            .withColumn(\"instance_name\", regexp_replace(\"instance_name\",\"-d\",\"d\"))\n",
    "            .withColumn(\"host_name\", regexp_replace(\"host_name\",\"\\['\",\"\"))\n",
    "            .withColumn(\"host_name\", regexp_replace(\"host_name\",\"'\\]\",\"\"))\n",
    "\n",
    "            #.filter(col(\"timestamp\").isNotNull())\n",
    "\n",
    "            # .filter(col(\"instance_name\") == \"daasebusgexd93 - RHEL8\") #testing 3/3/23\n",
    "\n",
    "        )}\n",
    "\n",
    "        tenant_dict[env]['Raw'].printSchema()\n",
    "\n",
    "    main_dict[tenant] = tenant_dict\n",
    "\n",
    "\n",
    "groupby_col_list = [\n",
    "    col(\"instance_id\"),\n",
    "    col(\"instance_name\"),\n",
    "    col(\"environment\"),\n",
    "    col(\"instance_type\"),\n",
    "    col(\"host_name\")\n",
    "\n",
    "]\n",
    "\n",
    "agg_list = [\n",
    "    count(\"*\").alias(\"Recorded Uptime\"),\n",
    "    min(col(\"timestamp\")).alias(\"First Recorded Timestamp\"),\n",
    "    max(col(\"timestamp\")).alias(\"Last Recorded Timestamp\"),\n",
    "    when(\n",
    "        col(\"Environment\") == \"daas_prod\",\n",
    "        patch_start_dt\n",
    "    ).alias(\"Planned Patching Start\"),\n",
    "    when(\n",
    "        col(\"Environment\") == \"daas_prod\",\n",
    "        patch_end_dt\n",
    "    ).alias(\"Planned Patching End\"),\n",
    "    round(((max(col(\"timestamp\")).cast(\"long\") - min(col(\"timestamp\")).cast(\"long\") + 300) / 3600),rnd).alias(\"Total Availability (Hours)\"),\n",
    "    when(\n",
    "        ((col(\"Environment\") == \"daas_prod\") & ((min(col(\"timestamp\")) < patch_start_dt) & (max(col(\"timestamp\")) > patch_end_dt)))\n",
    "\n",
    "        ,total_patch_time\n",
    "\n",
    "    ).alias(\"Patching Downtime (Hours)\"),\n",
    "\n",
    "    (\n",
    "        (\n",
    "            (max(col(\"timestamp\")).cast(\"long\") - min(col(\"timestamp\")).cast(\"long\") + 300) / 3600\n",
    "\n",
    "        ) - when(\n",
    "\n",
    "                (col(\"Environment\") == \"daas_prod\") &\n",
    "\n",
    "                (\n",
    "                    (min(col(\"timestamp\")) < patch_start_dt) &\n",
    "\n",
    "                    (max(col(\"timestamp\")) > patch_end_dt)\n",
    "\n",
    "                ),\n",
    "\n",
    "                total_patch_time\n",
    "\n",
    "        ).otherwise(0)\n",
    "\n",
    "    ).alias(\"Total Availability Excluding Patching (Hours)\")\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tenant in main_dict:\n",
    "    for env in main_dict[tenant]:\n",
    "        print(tenant + \" \" + env)\n",
    "        temp_raw_df = (main_dict[tenant][env]['Raw']) #.join(daas_zone_df, \"instance_id\",\"left\"))\n",
    "        temp_metrics_df = (main_dict[tenant][env]['Raw']\n",
    "        .orderBy(*groupby_col_list,col(\"timestamp\"))\n",
    "        .groupBy(groupby_col_list + [lit(tenant).alias(\"Program\")])\n",
    "        .agg(*agg_list)\n",
    "\n",
    "        )\n",
    "\n",
    "        if('main_df' not in locals()):\n",
    "            raw_df = temp_raw_df\n",
    "            main_df = temp_metrics_df\n",
    "        else:\n",
    "            main_df = main_df.unionByName(temp_metrics_df,allowMissingColumns=True)\n",
    "            raw_df = main_df.unionByName(temp_raw_df,allowMissingColumns=True)\n",
    "\n",
    "\n",
    "main_df = (main_df.withColumn('Recorded Uptime',round(col('Recorded Uptime')/12, rnd))\n",
    "            .withColumnRenamed(\"instance_id\", \"Instance ID\")\n",
    "            .withColumnRenamed(\"instance_name\", \"Instance Name\")\n",
    "            .withColumn(\"Instance Name\",\n",
    "                when(col(\"Instance Name\").startswith(\"=\"), expr(\"substring('Instance Name', 2, length('Instance Name') - 1)\"))\n",
    "                .when(col(\"Instance Name\").endswith(\"-\"), expr(\"substring('Instance Name', 1, length('Instance Name') - 1)\"))\n",
    "                .otherwise(col(\"Instance Name\")))\n",
    "            .withColumnRenamed(\"environment\", \"Environment\")\n",
    "            .withColumn(\n",
    "                \"IP Address\",\n",
    "                when(col(\"host_name\") == '[None]', None)\n",
    "                .when(\n",
    "                    col(\"host_name\").rlike(r\"ip-\\d{1,3}-\\d{1,3}-\\d{1,3}-\\d{1,3}\"),\n",
    "                    regexp_replace(\n",
    "                        regexp_extract(col(\"host_name\"), r\"ip-(\\d{1,3}-\\d{1,3}-\\d{1,3}-\\d{1,3})\", 1),\n",
    "                        \"-\", \".\"\n",
    "                    )\n",
    "\n",
    "                )\n",
    "                .otherwise(None)\n",
    "\n",
    "            )\n",
    "\n",
    "            .withColumnRenamed(\"instance_type\", \"Instance Type\")\n",
    "            .withColumn(\n",
    "                \"Patching Downtime (Hours)\",\n",
    "                when(col(\"Planned Patching End\").isNotNull(),\n",
    "                    round((col(\"Planned Patching End\").cast(\"long\") - col(\"Planned Patching Start\").cast(\"long\")) / 3600,rnd)\n",
    "                ).otherwise(None)\n",
    "\n",
    "            )\n",
    "\n",
    "            .withColumn(\"First Recorded Timestamp\", date_format(col(\"First Recorded Timestamp\"), \"yyyy-MM-dd HH:mm:ss\"))\n",
    "            .withColumn(\"Last Recorded Timestamp\", date_format(col(\"Last Recorded Timestamp\"), \"yyyy-MM-dd HH:mm:ss\"))\n",
    "            .withColumn(\"Planned Patching Start\", date_format(col(\"Planned Patching Start\"), \"yyyy-MM-dd HH:mm:ss\"))\n",
    "            .withColumn(\"Planned Patching End\", date_format(col(\"Planned Patching End\"), \"yyyy-MM-dd HH:mm:ss\"))\n",
    "            .withColumn(\n",
    "                \"Total Availability Excluding Patching (Hours)\",\n",
    "                when(col(\"Instance Name\").endswith('api'),lit('N/A'))\n",
    "                .otherwise(round(col(\"Total Availability Excluding Patching (Hours)\"),rnd))\n",
    "\n",
    "            )\n",
    "            .withColumn(\n",
    "                \"Unplanned Downtime\",\n",
    "                when(col(\"Instance Name\").endswith('api'),lit('N/A'))\n",
    "                .otherwise((round(col(\"Total Availability Excluding Patching (Hours)\"),2) - round(col(\"Recorded Uptime\"), 2)))\n",
    "\n",
    "            )\n",
    "            .withColumn(\n",
    "                \"Percent Uptime\",\n",
    "                when(col(\"Total Availability Excluding Patching (Hours)\") == 'N/A', lit('N/A'))\n",
    "                .otherwise(round(col(\"Recorded Uptime\") / col(\"Total Availability Excluding Patching (Hours)\"), 2))\n",
    "\n",
    "            )\n",
    "        )\n",
    "\n",
    "main_df.coalesce(1).write.option(\"header\",\"true\").mode(\"overwrite\").csv(dest_report) #.filter(((col(\"environment\") == 'daas_prod') | (col(\"environment\") == 'daas_api_prod')))\n",
    "\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
